spark.armada.queue georgej
spark.armada.scheduling.namespace gold-georgej

#spark.armada.lookouturl https://lookout.cluster1.armada-staging.staging.zone/
# To connect to the prod cluster1 cluster UI uncomment this line:
spark.armada.lookouturl https://lookout.cluster1.armada.staging.zone/
# To connect to the prod cluster2 cluster UI uncomment this line:
#spark.armada.lookouturl https://lookout.cluster2.armada.staging.zone/

spark.armada.jobSetId armada-spark
spark.armada.internalUrl=unused
#spark.executor.instances 3
spark.jars.ivy /tmp/.ivy
spark.armada.scheduling.nodeUniformity gr/clustername

spark.hadoop.fs.s3a.path.style.access True
spark.eventLog.enabled false


spark.armada.driver.limit.memory 4000Mi
spark.armada.driver.request.memory 4000Mi
spark.armada.executor.limit.memory 51814Mi
spark.armada.executor.request.memory 51814Mi
spark.armada.executor.limit.cores 2
spark.armada.executor.request.cores 2


# spark.authenticate=true
# spark.driver.log.dfsDir=/spark/spark3/driverLogs
# spark.driver.log.persistToDfs.enabled=false
spark.eventLog.enabled=false
spark.eventLog.dir=s3a://playpen-georgej/benchmark
# spark.eventLog.rolling.enabled=true
spark.eventLog.rolling.maxFileSize=128m
# spark.eventLog.compress=true

spark.io.encryption.enabled=false
spark.lineage.enabled=true

# spark.network.crypto.enabled=true
# spark.network.crypto.keyLength=256
# spark.network.crypto.keyFactoryAlgorithm=PBKDF2WithHmacSHA256


spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.sql.shuffle.partitions=1000

spark.ui.enabled=true
spark.ui.killEnabled=true

# shuffle defaults
spark.shuffle.file.buffer=1m
spark.shuffle.io.backLog=8192
spark.shuffle.io.serverThreads=128
spark.shuffle.registration.maxAttempts=5
spark.shuffle.registration.timeout=30000
spark.shuffle.service.index.cache.size=2048m
spark.shuffle.unsafe.file.output.buffer=5m
spark.io.compression.lz4.blockSize=128kb
spark.unsafe.sorter.spill.reader.buffer.size=1m
spark.dynamicAllocation.shuffleTracking.enabled=false

spark.sql.legacy.parquet.nanosAsLong=true

# dynamic allocation thresholds
spark.dynamicAllocation.enabled=false
spark.dynamicAllocation.executorIdleTimeout=60
spark.dynamicAllocation.schedulerBacklogTimeout=1
spark.dynamicAllocation.cachedExecutorIdleTimeout=240
spark.dynamicAllocation.initialExecutors=2
spark.dynamicAllocation.minExecutors=1
spark.dynamicAllocation.maxExecutors=400


#spark.master=k8s://staging
#spark.kubernetes.context=staging
spark.submit.deployMode=client

# default optimization
spark.sql.autoBroadcastJoinThreshold=26214400
spark.executorEnv.MKL_NUM_THREADS=1
spark.yarn.appMasterEnv.OPENBLAS_NUM_THREADS=1
spark.executorEnv.OPENBLAS_NUM_THREADS=1
spark.rss.estimate.task.concurrency.dynamic.factor=1.0
spark.rss.client.io.compression.codec=lz4
spark.sql.execution.arrow.pyspark.enabled=true

## View ACL's
spark.acls.enable=true
spark.ui.view.acls.groups=staging

# spark.kubernetes.driver.pod.featureSteps=org.apache.spark.deploy.k8s.features.LiveWebUiFeatureStep,org.apache.spark.deploy.k8s.features.YunikornQueueFeatureStep,org.apache.spark.deploy.k8s.features.FfsMountFeatureStep
# spark.kubernetes.executor.pod.featureSteps=org.apache.spark.deploy.k8s.features.YunikornQueueExecutorFeatureStep,org.apache.spark.deploy.k8s.features.FfsMountExecutorFeatureStep
spark.ui.filters=filter
spark.oauth.clientid=staging
spark.user.groups.mapping=staging
spark.hadoop.hadoop.security.group.mapping.ldap.url=ldaps://staging.zone
spark.hadoop.hadoop.security.group.mapping.ldap.base="ou=Root,dc=staging,dc=zone"
spark.hadoop.hadoop.security.group.mapping.ldap.userbase="ou=Users,ou=Accounts,ou=Active,ou=Root,dc=staging,dc=zone"
spark.hadoop.hadoop.security.group.mapping.ldap.groupbase="ou=Groups,ou=Active,ou=Root,dc=staging,dc=zone"

spark.executor.instances=100

## Monitoring
spark.ui.prometheus.enabled=true
spark.executor.processTreeMetrics.enabled=true
spark.metrics.appStatusSource.enabled=true
spark.scheduler.listenerbus.eventqueue.capacity=100000

## UI config
spark.ui.retainedJobs=999999
spark.ui.retainedStages=999999
spark.ui.retainedTasks=999999
spark.worker.ui.retainedExecutors=999999
spark.sql.ui.retainedExecutions=999999
spark.ui.retainedDeadExecutors=999999
spark.ui.timeline.executors.maximum=999999
#spark.ui.custom.executor.log.url="https://spark-logs.spark-system.staging.zone/apps/{{APP_ID}}#executor&id={{EXECUTOR_ID}}"
#spark.ui.custom.driver.log.url="https://spark-logs.spark-system.staging.zone/apps/{{APP_ID}}"

# # Uniffle config
# # this needs to be disabled when using an external shuffle service
spark.shuffle.service.enabled=false
# spark.shuffle.manager=org.apache.spark.shuffle.RssShuffleManager
# spark.rss.coordinator.quorum=10.114.40.1:19989,
# spark.rss.estimate.task.concurrency.dynamic.factor=1.0
# spark.sql.adaptive.localShuffleReader.enabled=false
# spark.rss.client.shuffle.data.distribution.type=LOCAL_ORDER
# spark.rss.client.unregister.request.timeout.sec=60000
# spark.rss.client.unregister.thread.pool.size=120
# # make uniffle aware that the spark cluster is preemptible
# spark.rss.cluster.preemptible=true

## Kubernetes
spark.kubernetes.container.image=image
# spark.kubernetes.authenticate.driver.serviceAccountName=spark
spark.kubernetes.driver.label.appregistry.gresearch.co.uk/app-id=1
spark.kubernetes.driver.label.node-butcher.gresearch.co.uk/dont-kill=this-node
spark.kubernetes.executor.label.appregistry.gresearch.co.uk/app-id=1
spark.kubernetes.driver.label.spark=true
# spark.kubernetes.executor.podTemplateFile=${env:SPARK_CONF_DIR}/pod-template.yaml
# spark.kubernetes.driver.podTemplateFile=${env:SPARK_CONF_DIR}/pod-template-driver.yaml
spark.kubernetes.container.image.pullPolicy=IfNotPresent
spark.kubernetes.file.upload.path=hdfs:///tmp/
spark.executorEnv.KRB5_TRACE=/dev/stdout
spark.kubernetes.allocation.maxPendingPods=30

## Custom Kubernetes
# spark.kubernetes.ffs.probe.enabled=true

## S3 Configuration
spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.path.style.access=true
spark.hadoop.fs.s3a.fast.upload=true
spark.hadoop.fs.s3a.committer.name=directory
spark.hadoop.fs.s3a.multipart.threshold=320M
spark.hadoop.fs.s3a.multipart.size=32M
spark.hadoop.fs.s3a.connection.maximum=10000
spark.hadoop.fs.s3a.threads.max=5000
#spark.sql.parquet.output.committer.class=org.apache.spark.internal.io.cloud.BindingParquetOutputCommitter
#spark.sql.sources.commitProtocolClass=org.apache.spark.internal.io.cloud.PathOutputCommitProtocol

## override the below to allow allSilver users to view logs in elk
spark.kubernetes.driver.label.spark-logs-secure=true
spark.kubernetes.executor.label.spark-logs-secure=true

spark.driver.cores=2
spark.driver.memory=18g

spark.executor.cores=5
spark.executor.memory=46g

#added to force tests to wait for executors
spark.scheduler.maxRegisteredResourcesWaitingTime=3000s
spark.scheduler.minRegisteredResourcesRatio=1.0
spark.history.fs.logDirectory=/tmp/log
